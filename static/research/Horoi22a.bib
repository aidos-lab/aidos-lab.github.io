@inproceedings{Horoi22a,
	abstract = {Recent work has established clear links between the
generalization performance of trained neural networks
and the geometry of their loss landscape near the
local minima to which they converge. This suggests
that qualitative and quantitative examination of the
loss landscape geometry could yield insights about
neural network generalization performance during
training. To this end, researchers have proposed
visualizing the loss landscape through the use of
simple dimensionality reduction techniques. However,
such visualization methods have been limited by their
linear nature and only capture features in one or two
dimensions, thus restricting sampling of the loss
landscape to lines or planes. Here, we expand and
improve upon these in three ways. First, we present
a novel ``jump and retrain'' procedure for sampling
relevant portions of the loss landscape. We show that
the resulting sampled data holds more meaningful
information about the network's ability to generalize.
Next, we show that non-linear dimensionality reduction
of the jump and retrain trajectories via PHATE,
a trajectory and manifold-preserving method, allows us
to visualize differences between networks that are
generalizing well vs poorly. Finally, we combine PHATE
trajectories with a computational homology
characterization to quantify trajectory differences.},
	address = {Cham, Switzerland},
	archiveprefix = {arXiv},
	author = {Stefan Horoi and Jessie Huang and Bastian Rieck and Guillaume Lajoie and Guy Wolf and Smita Krishnaswamy},
	author+an = {1=first; 2=first; 3=highlight; 5=last; 6=last},
	booktitle = {Advances in Intelligent Data Analysis XX},
	doi = {10.1007/978-3-031-01333-1_14},
	editor = {Bouadi, Tassadit and Fromont, Elisa and H{\"u}llermeier, Eyke},
	eprint = {2102.00485},
	isbn = {978-3-031-01333-1},
	pages = {171--184},
	primaryclass = {cs.LG},
	publisher = {Springer},
	title = {Exploring the Geometry and Topology of Neural Network Loss Landscapes},
	year = {2022},
}
